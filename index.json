[{"authors":["admin"],"categories":null,"content":"Lorenzo Perini is a research scientist in the Central Applied Science (CAS) team at Meta focusing on probabilistic machine learning. He earned his BSc in Mathematics from the University of Florence in 2017, and his MSc in Mathematical Engineering from Politecnico di Torino in 2019, where he specialized in data statistics and network optimization. His Master\u0026rsquo;s thesis, conducted in collaboration with Tierra S.p.A., explored predictive maintenance using Hidden Markov Models and Autoencoders.\nIn 2019, Lorenzo began his PhD in the DTAI Lab at KU Leuven under Prof. Dr. Jesse Davis, focusing on uncertainty quantification and anomaly detection. His work has been recognized with several fellowships, including a PhD fellowship from the Research Foundation – Flanders (FWO) and the Scientific Prize Gustave Boël-Sofina Fellowship for talented researchers for a long stay abroad. During his PhD, he was a visiting researcher at the University of Helsinki and completed an internship at Bosch Center for Artificial Intelligence (BCAI).\nLorenzo has published papers in prestigious conferences such as NeurIPS, ICML, KDD, AAAI, IJCAI, and ECAI. His main research interests include Uncertainty Quantification and Anomaly Detection, often with the introduction of the human-in-the-loop, e.g. via Active Learning and Learning to Reject. In March 2024, he defended his doctoral dissertation titled operational, uncertainty-aware, and reliable anomaly detection.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://lorenzo-perini.github.io/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"Lorenzo Perini is a research scientist in the Central Applied Science (CAS) team at Meta focusing on probabilistic machine learning. He earned his BSc in Mathematics from the University of Florence in 2017, and his MSc in Mathematical Engineering from Politecnico di Torino in 2019, where he specialized in data statistics and network optimization. His Master\u0026rsquo;s thesis, conducted in collaboration with Tierra S.p.A., explored predictive maintenance using Hidden Markov Models and Autoencoders.","tags":null,"title":"Lorenzo Perini","type":"authors"},{"authors":["Lorenzo Perini"],"categories":[],"content":"","date":1711929600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1711929600,"objectID":"ecd92f6d7c0c32175d9d5333dba47fe0","permalink":"https://lorenzo-perini.github.io/publication/phdthesis/","publishdate":"2024-04-01T00:00:00Z","relpermalink":"/publication/phdthesis/","section":"publication","summary":"Anomaly detection methods aim to identify examples that do not follow the expected behavior. For various reasons, anomaly detection is typically tackled by using unsupervised approaches that assign real-valued anomaly scores based on various heuristics. For instance, one can assume that anomalies fall in low-density regions and compute the negative log-likelihood as anomaly score. Because anomaly scores are often hard to interpret, practitioners need class labels (i.e., anomaly yes/no) for decision-making. That is, one needs to set a proper decision threshold to flag high-score examples as anomalies. However, finding a threshold requires having access to labeled examples for evaluating the quality of the predicted class labels, which is unfeasible in unsupervised anomaly detection. Moreover, existing literature has focused mainly on measuring the quality of the anomaly scores through ranking-based metrics (e.g., AUROC), which largely ignores the problem of how to derive class predictions. Here, we fill this gap by proposing three novel approaches to transform scores into class predictions. Given a detector’s class predictions, a natural question is: how likely does a prediction change when learning a detector on training data that is subject to slight perturbation? Because unsupervised detectors cannot refine the decision boundary by leveraging labeled examples, they tend to have high uncertainty in predictions. That is, slight changes in the training set often would yield a different decision boundary which, in turn, would flip some test examples’ class prediction. This uncertainty makes it hard to deploy a detector in real-world applications as it deteriorates the practitioner’s trust in its crucial predictions. Because existing literature largely ignores this problem, we fill this gap by proposing an unsupervised approach to quantify a detector’s uncertainty in predictions. While quantifying uncertainty is essential, practitioners also need a reliable way to assess whether they can trust a detector’s prediction. That is, one needs to answer the question: is the detector’s uncertainty low enough to rely on its prediction? This falls into the field of Learning with Rejection, where the model is allowed to abstain (i.e., defer the decision, or “reject” it) when its uncertainty is too high, such that practitioners can trust its output whenever it makes a prediction. Traditionally, learning with rejection approaches rely on evaluating the risk (or, equivalently, the cost) of making mispredictions to design the rejection mechanism, which requires labeled examples. Because no unsupervised method for rejection exists, we fill this gap and propose the first unsupervised anomaly detection algorithm with rejection.","tags":["Anomaly Detection","PU Learning","Uncertainty Quantification","Bayesian Learning","Transfer Learning","Active Learning","Learning to Reject","Unsupervised Learning"],"title":"Operational, Uncertainty-Aware, and Reliable Anomaly Detection","type":"publication"},{"authors":["Kilian Hendrickx","Lorenzo Perini","Dries Van der Plas","Wannes Meert","Jesse Davis"],"categories":[],"content":"","date":1711670400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1711670400,"objectID":"c8c0606d84b0c95883d7afb9e8d4e225","permalink":"https://lorenzo-perini.github.io/publication/mach_rejection/","publishdate":"2024-03-29T00:00:00Z","relpermalink":"/publication/mach_rejection/","section":"publication","summary":"Machine learning models always make a prediction, even when it is likely to be inaccurate. This behavior should be avoided in many decision support applications, where mistakes can have severe consequences. Albeit already studied in 1970, machine learning with a reject option recently gained interest. This machine learning subfield enables machine learning models to abstain from making a prediction when likely to make a mistake. This survey aims to provide an overview on machine learning with a reject option. We introduce the conditions leading to two types of rejection, ambiguity and novelty rejection. Moreover, we define the existing architectures for models with a reject option, describe the standard learning strategies to train such models and relate traditional machine learning techniques to rejection. Additionally, we review strategies to evaluate a model's predictive and rejective quality. Finally, we provide examples of relevant application domains and show how machine learning with rejection relates to other machine learning research areas.","tags":["Machine Learning with Rejection","Supervised Learning","Trustworthy Machine Learning"],"title":"Machine Learning with a Reject Option: A survey","type":"publication"},{"authors":["Lorenzo Perini","Jesse Davis"],"categories":[],"content":"","date":1696118400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1696118400,"objectID":"97b8f5f491b5e1ba68ae85c49d02486d","permalink":"https://lorenzo-perini.github.io/publication/neurips23/","publishdate":"2023-10-01T00:00:00Z","relpermalink":"/publication/neurips23/","section":"publication","summary":"Anomaly detection aims at detecting unexpected behaviours in the data. Because anomaly detection is usually an unsupervised task, traditional anomaly detectors learn a decision boundary by employing heuristics based on intuitions, which are hard to verify in practice. This introduces some uncertainty, especially close to the decision boundary, that may reduce the user trust in the detector's predictions. A way to combat this is by allowing the detector to reject examples with high uncertainty (Learning to Reject). This requires employing a confidence metric that captures the distance to the decision boundary and setting a rejection threshold to reject low-confidence predictions. However, selecting a proper metric and setting the rejection threshold without labels are challenging tasks. In this paper, we solve these challenges by setting a constant rejection threshold on the stability metric computed by ExCeeD. Our insight relies on a theoretical analysis of such a metric. Moreover, setting a constant threshold results in strong guarantees: we estimate the test rejection rate, and derive a theoretical upper bound for both the rejection rate and the expected prediction cost. Experimentally, we show that our method outperforms some metric-based methods.","tags":["Anomaly Detection","Learning to Reject","Unsupervised Learning"],"title":"Unsupervised Anomaly Detection with Rejection","type":"publication"},{"authors":["Laurens Devos","Lorenzo Perini","Wannes Meert","Jesse Davis"],"categories":[],"content":"","date":1694995200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1694995200,"objectID":"c28f3acf314bf0e75d2d0c4ec464bc85","permalink":"https://lorenzo-perini.github.io/publication/ecml2023a/","publishdate":"2023-09-01T00:00:00Z","relpermalink":"/publication/ecml2023a/","section":"publication","summary":"Tree ensembles are powerful models that are widely used. However, they are susceptible to evasion attacks where an adversary purposely constructs an adversarial example in order to elicit a misprediction from the model. This can degrade performance and erode a user’s trust in the model. Typically, approaches try to alleviate this problem by verifying how robust a learned ensemble is or robustifying the learning process. We take an alternative approach and attempt to detect adversarial examples in a post-deployment setting. We present a novel method for this task that works by analyzing an unseen example’s output configuration, which is the set of leaves activated by the example in the ensemble’s constituent trees. Our approach works with any additive tree ensemble and does not require training a separate model. We evaluate our approach on three different tree ensemble learners. We empirically show that our method is currently the best adversarial detection method for tree ensembles.","tags":["Evasion attack detection","Tree ensembles"],"title":"Detecting Evasion Attacks in Deployed Tree Ensembles","type":"publication"},{"authors":["Timo Martens","Lorenzo Perini","Jesse Davis"],"categories":[],"content":"","date":1694995200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1694995200,"objectID":"2d470ba3f02c01d7262ec385089b9df9","permalink":"https://lorenzo-perini.github.io/publication/ecml2023b/","publishdate":"2023-09-01T00:00:00Z","relpermalink":"/publication/ecml2023b/","section":"publication","summary":"Anomaly detection aims at detecting examples that do not conform to normal behavior. Increasingly, anomaly detection is being approached from a semi-supervised perspective where active learning is employed to acquire a small number of strategically selected labels. However, because anomalies are not always well-understood events, the user may be uncertain about how to label certain instances. Thus, one can relax this request and allow the user to provide soft labels (i.e., probabilistic labels) that represent their belief that a queried example is anomalous. These labels are naturally noisy due to the user’s inherent uncertainty in the label and the fact that people are known to be bad at providing well-calibrated probability instances. To cope with these challenges, we propose to exploit a Gaussian Process to learn from actively acquired soft labels in the context of anomaly detection. This enables leveraging information about nearby examples to smooth out possible noise. Empirically, we compare our proposed approach to several baselines on 21 datasets and show that it outperforms them in the majority of experiments.","tags":["Anomaly Detection","Probabilistic Labels","Noisy Labels"],"title":"Semi-Supervised Learning from Active Noisy Soft Labels for Anomaly Detection","type":"publication"},{"authors":["Lorenzo Perini","Vincent Vercruyssen","Jesse Davis"],"categories":[],"content":"","date":1691193600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1691193600,"objectID":"f98737cb9d38116c09ef1c913a796517","permalink":"https://lorenzo-perini.github.io/publication/kdd23/","publishdate":"2023-08-05T00:00:00Z","relpermalink":"/publication/kdd23/","section":"publication","summary":"In the multi-instance learning (MIL) setting instances are grouped together into bags. Labels are provided only for the bags and not on the level of individual instances. A positive bag label means that at least one instance inside the bag is positive, while a negative bag label restricts all the instances in the bag to be negative. MIL data naturally arises in many contexts, such as anomaly detection, where labels are rare and costly, and one often ends up annotating the label for sets of instances. Moreover, in many real-world anomaly detection problems, only positive labels are collected because they usually represent critical events. Such a setting, where only positive labels are provided along with unlabeled data, is called Positive and Unlabeled (PU) learning. Despite being useful for several use cases, there is no work dedicated to learning from positive and unlabeled data in a multi-instance setting for anomaly detection. Therefore, we propose the first method that learns from PU bags in anomaly detection. Our method uses an autoencoder as an underlying anomaly detector. We alter the autoencoder’s objective function and propose a new loss that allows it to learn from positive and unlabeled bags of instances. We theoretically analyze this method. Experimentally, we evaluate our method on 30 datasets and show that it performs better than multiple baselines adapted to work in our setting.","tags":["Multi-Instance Learning","PU Learning","Anomaly Detection"],"title":"Learning from Positive and Unlabeled Multi-Instance Bags in Anomaly Detection","type":"publication"},{"authors":["Lorenzo Perini","Paul-Christian Buerkner","Arto Klami"],"categories":[],"content":"","date":1689984000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689984000,"objectID":"d65d20f9b5d1e1b3cd6d9879f48bc665","permalink":"https://lorenzo-perini.github.io/publication/icml23/","publishdate":"2023-09-02T00:00:00Z","relpermalink":"/publication/icml23/","section":"publication","summary":"Anomaly detection methods identify examples that do not follow the expected behaviour, typically in an unsupervised fashion, by assigning real-valued anomaly scores to the examples based on various heuristics. These scores need to be transformed into actual predictions by thresholding so that the proportion of examples marked as anomalies equals the expected proportion of anomalies, called contamination factor. Unfortunately, there are no good methods for estimating the contamination factor itself. We address this need from a Bayesian perspective, introducing a method for estimating the posterior distribution of the contamination factor for a given unlabeled dataset. We leverage several anomaly detectors to capture the basic notion of anomalousness and estimate the contamination using a specific mixture formulation. Empirically on 22 datasets, we show that the estimated distribution is well-calibrated and that setting the threshold using the posterior mean improves the detectors’ performance over several alternative methods.","tags":["Anomaly Detection","Bayesian Methods","Unsupervised Learning"],"title":"Estimating the Contamination Factor’s Distribution in Unsupervised Anomaly Detection","type":"publication"},{"authors":[],"categories":null,"content":"","date":1676291400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1676291400,"objectID":"d3f6c7909db7745f1037e0cb11e5a977","permalink":"https://lorenzo-perini.github.io/talk/aaai/","publishdate":"2023-02-13T13:30:00+01:00","relpermalink":"/talk/aaai/","section":"talk","summary":"","tags":["Uncertainty Quantification","Anomaly Detection","Active Learning","Learning to Reject"],"title":"How to Allocate your Label Budget? Choosing between Active Learning and Learning to Reject in Anomaly Detection","type":"talk"},{"authors":[],"categories":[],"content":"","date":1672531200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1672531200,"objectID":"2149f0c88b157e0f8ec0707bfa3810de","permalink":"https://lorenzo-perini.github.io/thesis/5year/second/","publishdate":"2023-01-01T00:00:00Z","relpermalink":"/thesis/5year/second/","section":"thesis","summary":"This thesis explores a novel active learning strategy for deep semi-supervised models with rejection, addressing classification tasks where mispredictions carry severe consequences and labeled data are scarce. By integrating active learning, semi-supervised learning, and learning with rejection, the proposed approach enables models to reject uncertain predictions and learn effectively from unlabeled data. The primary challenge lies in adapting active learning to models with rejection, as traditional strategies focus solely on improving predictive accuracy without accounting for the need to identify rejectable instances. To address this, the proposed strategy dynamically balances two complementary approaches. The first employs a cost-based framework to select data points likely to reduce the overall cost of mispredictions and rejections. The second approach mitigates sampling bias to enhance the robustness of the active learning process. Extensive experiments on five real-world datasets demonstrate the strategy’s effectiveness, achieving better and more robust performance compared to uncertainty sampling and random sampling baselines across various scenarios. These results underscore the potential of the proposed strategy to improve the efficiency and reliability of deep semi-supervised models with rejection.","tags":[],"title":"Better be cautious when asking hard questions! Developing an active learning strategy for semi-supervised models with rejection","type":"thesis"},{"authors":[],"categories":[],"content":"","date":1672531200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1672531200,"objectID":"2d1524173b6de84da026d42a261c152b","permalink":"https://lorenzo-perini.github.io/thesis/5year/third/","publishdate":"2023-01-01T00:00:00Z","relpermalink":"/thesis/5year/third/","section":"thesis","summary":"This thesis investigates a novel approach for data acquisition in anomaly detection, a critical field in machine learning focused on identifying unusual patterns in datasets. The goal is to develop a semi-supervised anomaly detection model that learns from pairwise ranked data, thereby simplifying the expert’s labeling process. By combining semi-supervised anomaly detection with supervised learning techniques from the learning-to-rank domain, the proposed method, SSPS, is tested on various benchmark datasets to evaluate its predictive performance. The results demonstrate that SSPS competes with state-of-the-art learning-to-rank algorithms applied to anomaly detection tasks, outperforming them on datasets where supervised learning struggles. This research suggests that ranking-based learning could offer a promising alternative for anomaly detection, particularly in scenarios where normal data points can be ranked relative to each other, such as credit card fraud detection and machine failure. The findings lay the groundwork for future research in semi-supervised pairwise anomaly detection as pairwise anomaly datasets become available.","tags":[],"title":"It may not be wrong but is definitely more anomalous than the others! Anomaly Detection with User Feedback Ranking","type":"thesis"},{"authors":[],"categories":[],"content":"","date":1672531200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1672531200,"objectID":"46fedc928ae6b2e933e22c3101b9b608","permalink":"https://lorenzo-perini.github.io/thesis/5year/first/","publishdate":"2023-01-01T00:00:00Z","relpermalink":"/thesis/5year/first/","section":"thesis","summary":"Anomaly detection aims to identify unexpected events, known as anomalies, that deviate from normal behavior and often represent critical occurrences. While typically addressed in an unsupervised manner, anomaly detection can benefit from weak supervision to reduce labeling costs. Multi-Instance Learning (MIL) provides a framework for weakly supervised anomaly detection by organizing data into labeled sets (bags) of instances, where a bag is anomalous if at least one instance is anomalous. In scenarios where labels are expensive, Active Learning strategies can optimize instance selection for labeling. However, standard strategies may be suboptimal in MIL settings due to varying distributions across bags. To address this, we propose the Aligning Multi-Instance Bandits (AMIB) method, which aligns normal instances across bags to follow a common distribution. AMIB combines a Multi-Armed Bandits approach for bag selection with Uncertainty Sampling for instance querying. Experimental results indicate that AMIB competes with standard Active Learning strategies at the instance level, particularly when anomalous instances overlap with normal ones across bags. However, at the bag level, AMIB demonstrates poor performance, yielding results comparable to a random classifier. These findings highlight AMIB's potential and limitations, offering insights for further research in MIL-based Active Learning for anomaly detection.","tags":[],"title":"What do slot machines have in common with Active Learning? Finding the high-reward instances in Multi-Instance Learning","type":"thesis"},{"authors":["Vincent Vercruyssen","Lorenzo Perini","Wannes Meert","Jesse Davis"],"categories":[],"content":"","date":1663113600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1663113600,"objectID":"8658861ec12c35323e6629b4d1954b9c","permalink":"https://lorenzo-perini.github.io/publication/ecml2022/","publishdate":"2022-08-21T12:25:53+02:00","relpermalink":"/publication/ecml2022/","section":"publication","summary":" Active learning aims to ease the burden of collecting large amounts of annotated data by intelligently acquiring labels during the learning process that will be most helpful to learner. Current active learning approaches focus on learning from a single dataset. However, a common setting in practice requires simultaneously learning models from multiple datasets, where each dataset requires a separate learned model. This paper tackles the less-explored multi-domain active learning setting. We approach this from the perspective of multi-armed bandits and propose the active learning bandits (Alba) method, which uses bandit methods to both explore and exploit the usefulness of querying a label from different datasets in subsequent query rounds. We evaluate our approach on a benchmark of 7 datasets collected from a retail environment, in the context of a real-world use case of detecting anomalous resource usage. Alba outperforms existing active learning strategies, providing evidence that the standard active learning approaches are less suitable for the multi-domain setting.","tags":["Anomaly detection","Active Learning","Semi-Supervised Learning","Multi-Armed Bandit"],"title":"Multi-domain Active Learning for Semi-supervised Anomaly Detection","type":"publication"},{"authors":["Lorenzo Perini","Vincent Vercruyssen","Jesse Davis"],"categories":[],"content":"","date":1645488000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1645488000,"objectID":"b117a9af064017a90f34de16d6ab26cb","permalink":"https://lorenzo-perini.github.io/publication/aaai22/","publishdate":"2022-02-15T11:53:37+02:00","relpermalink":"/publication/aaai22/","section":"publication","summary":"Anomaly detection attempts to find examples in a dataset that do not conform to the expected behavior. Algorithms for this task assign an anomaly score to each example representing its degree of anomalousness. Setting a threshold on the anomaly scores enables converting these scores into a discrete prediction for each example. Setting an appropriate threshold is challenging in practice since anomaly detection is often treated as an unsupervised problem. A common approach is to set the threshold based on the dataset’s contamination factor, i.e., the proportion of anomalous examples in the data. While the contamination factor may be known based on domain knowledge, it is often necessary to estimate it by labeling data. However, many anomaly detection problems involve monitoring multiple related, yet slightly different entities (e.g., a fleet of machines). Then, estimating the contamination factor for each dataset separately by labeling data would be extremely time-consuming. Therefore, this paper introduces a method for transferring the known contamination factor from one dataset (the source domain) to a related dataset where it is unknown (the target domain). Our approach does not require labeled target data and is based on modeling the shape of the distribution of the anomaly scores in both domains. We theoretically analyze how our method behaves when the (biased) target domain anomaly score distribution converges to its true one. Empirically, our method outperforms several baselines on real-world datasets.","tags":["Transfer Learning","Unsupervised Learning","Anomaly Detection"],"title":"Transferring the Contamination Factor between Anomaly Detection Domains by Shape Similarity","type":"publication"},{"authors":[],"categories":[],"content":"","date":1640995200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1640995200,"objectID":"51d909a1826efd5c38b31410becffbc0","permalink":"https://lorenzo-perini.github.io/thesis/4year/second/","publishdate":"2022-01-01T00:00:00Z","relpermalink":"/thesis/4year/second/","section":"thesis","summary":"Anomaly detection models often rely on anomaly scores to make predictions, but these scores are difficult to interpret and compare, making it challenging to derive confidence in predictions. To address this, anomaly scores can be transformed into calibrated probabilities through a calibration map. While traditional calibration methods require labeled data, this reliance conflicts with the largely unsupervised nature of anomaly detection. This work introduces a novel semi-supervised calibration method that bridges this gap by combining two approaches. The first approach uses statistical insights to define the likely region of a good calibration map, minimizing the area where accurate predictions may reside. The second approach augments the limited labeled data by generating additional pseudo-labels for unlabeled items, enabling the application of supervised calibration techniques. The final method integrates these approaches, adjusting the calibration map from the second approach to fit within the boundaries set by the first. Evaluations on 15 anomaly score sets from various models and benchmark datasets reveal that while the proposed method does not consistently outperform existing calibration methods, it provides valuable insights into semi-supervised calibration. These approaches, individually or combined, form a solid foundation for advancing the calibration of anomaly scores in future research.","tags":[],"title":"It is likely not to be so likely! Semi-supervised calibration of anomaly scores","type":"thesis"},{"authors":[],"categories":[],"content":"","date":1640995200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1640995200,"objectID":"4dadbcfa2174f748c8799cd9ac2edc51","permalink":"https://lorenzo-perini.github.io/thesis/4year/first/","publishdate":"2022-01-01T00:00:00Z","relpermalink":"/thesis/4year/first/","section":"thesis","summary":"Anomaly detection involves identifying instances in data that deviate from expected patterns, typically by assigning anomaly scores to measure deviation. These scores, combined with thresholds, determine labels. However, the diverse scoring approaches used in current algorithms can hinder interpretability and reduce user trust in critical decisions. ExCeeD, the current state-of-the-art method for quantifying confidence in anomaly detection, has notable limitations: it depends on the true proportion of anomalies in the dataset and uses a discrete mapping of anomaly scores to confidence intervals. This thesis introduces the Lismont method, a novel approach to quantify confidence in anomaly detection. Unlike ExCeeD, the Lismont method calculates continuous confidence values without relying on the dataset’s true anomaly proportion. Additionally, we propose a new metric to evaluate the continuity of confidence methods. Experimental results demonstrate that the Lismont method improves performance over ExCeeD across various scenarios, offering enhanced confidence quantification and greater interpretability in anomaly detection tasks.","tags":[],"title":"This is critical and a lot is at stake. How can I trust the model? Quantifying the model uncertainty in anomaly detection","type":"thesis"},{"authors":["Jonas Soenen","Elia Van Wolputte","Lorenzo Perini","Vincent Vercruyssen","Wannes Meert","Jesse Davis","Hendrik Blockeel"],"categories":[],"content":"","date":1625270400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625270400,"objectID":"6564dc4c112d164dc939217606831b95","permalink":"https://lorenzo-perini.github.io/publication/odd21/","publishdate":"2021-07-03T00:00:00Z","relpermalink":"/publication/odd21/","section":"publication","summary":"Anomaly detection aims at finding observations in a dataset that do not conform to expected behavior. Researchers have proposed a large variety of anomaly detection algorithms and their performance is greatly affected by how a user sets each algorithm’s hyperparameters. However, the anomaly detection literature does not agree on how to set these hyperparameters when experimentally comparing different algorithms. Most papers compare either performance using “default” settings, or maximal performance under optimal settings. In this paper, we argue that both strategies fail to capture what practitioners are actually interested in: how well does the algorithm perform in practice? They are either too pessimistic, assuming no tuning, or unrealistically optimistic, assuming optimal tuning; and they often result in methodologically unsound and irreproducible comparisons between algorithms. We therefore propose to use a small validation set to tune an anomaly detector’s hyperparameters on a per dataset basis. We argue this is realistic, striking the balance between keeping the cost of acquiring labeled data low and selecting the hyperparameters in a fair, sound, and reproducible manner. We provide a theoretical lower bound on the validation set size based on probability of an anomaly detector achieving a higher area under the ROC curve than a random detector. Using a benchmark of 16 datasets, we experimentally show that different hyperparameter selection strategies lead to different conclusions about which algorithms perform better than others, and that using a small validation set is a practically feasible and principled way of tuning the hyperparameters for a given dataset.","tags":["Data Mining","Anomaly Detection","Outlier Detection"],"title":"The Effect of Hyperparameter Tuning on the Comparative Evaluation of Unsupervised Anomaly Detection Methods","type":"publication"},{"authors":[],"categories":null,"content":"","date":1614616200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1614616200,"objectID":"0f813efe7e3199da42662f1905bb4034","permalink":"https://lorenzo-perini.github.io/talk/polito/","publishdate":"2021-03-01T17:30:00+01:00","relpermalink":"/talk/polito/","section":"talk","summary":"","tags":["Uncertainty Quantification","Anomaly Detection"],"title":"Quantifying the Confidence of Anomaly Detectors in Their Example-Wise Predictions","type":"talk"},{"authors":[],"categories":[],"content":"","date":1612998000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1612998000,"objectID":"373ec5f4c4db1b82fe3f13e4d22704d2","permalink":"https://lorenzo-perini.github.io/award/fwo_fellowship/","publishdate":"2021-02-11T00:00:00+01:00","relpermalink":"/award/fwo_fellowship/","section":"award","summary":"PhD grant for the research project ''Measuring and Exploiting the Uncertainty in Anomaly Detection''.","tags":[],"title":"PhD Fellowship fundamental research (FWO)","type":"award"},{"authors":[],"categories":[],"content":"","date":1612998000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1612998000,"objectID":"f451c30dca3854993457fbb7574ace33","permalink":"https://lorenzo-perini.github.io/award/sb_fellowship/","publishdate":"2021-02-11T00:00:00+01:00","relpermalink":"/award/sb_fellowship/","section":"award","summary":"F.R.S.-FNRS \u0026 FWO grant for talented PhD students for a long research stay abroad.","tags":[],"title":"Scientific prize Gustave Boël-Sofina Fellowship","type":"award"},{"authors":[],"categories":[],"content":"","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609459200,"objectID":"f36bbe2cec6cb0d8ea86bbc69b349c2a","permalink":"https://lorenzo-perini.github.io/thesis/3year/second/","publishdate":"2021-01-01T00:00:00Z","relpermalink":"/thesis/3year/second/","section":"thesis","summary":"Detecting abnormal behaviors in real-world applications is critical for preventing dangerous situations. While anomaly detection has traditionally been treated as an unsupervised learning task due to the scarcity and cost of labeled data, the availability of limited labels has spurred the development of semi-supervised models that significantly enhance performance. Among these, tree-based models are a promising but underexplored approach due to the challenge of integrating labeled and unlabeled data during tree construction. This work introduces a novel semi-supervised tree-based model that leverages both labeled and unlabeled data to effectively partition the feature space, distinguishing normal samples from anomalies. The proposed method is evaluated on multiple benchmark datasets and compared against state-of-the-art algorithms. Results demonstrate that the model consistently outperforms unsupervised and semi-supervised baselines, highlighting its potential for robust anomaly detection in semi-supervised scenarios.","tags":[],"title":"Adaptive semi-supervised anomaly detection with any unsupervised prior","type":"thesis"},{"authors":[],"categories":[],"content":"","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609459200,"objectID":"0b09fd5d1350d3aa76076bcd1d473560","permalink":"https://lorenzo-perini.github.io/thesis/3year/third/","publishdate":"2021-01-01T00:00:00Z","relpermalink":"/thesis/3year/third/","section":"thesis","summary":"Anomaly detection is a machine learning task in which the goal is to detect the outliers in a given data set. In real-life applications, one usually has a label budget because collecting labels can be costly. This is where semi-supervised anomaly detection models come in. They are able to learn from a limited labeled data set and a larger set of unlabeled data. However, they usually assume only correct labels, but labeling data can be a very challenging task. Due to the small set of labeled data, noisy labels can have a detrimental effect on the model's accuracy. We introduce a new setting in which the human annotator is asked to provide a confidence score along with her labels. Subsequently, we propose a novel semi-supervised anomaly detection model that incorporates these confidence scores to become more robust against noisy labels. By simulating a human annotator, we can compare our model against the state of the art on multiple benchmark data sets. We find empirical evidence for the robustness of our model, but have to conclude that it lacks other desired properties such as, e.g. the speed at which it learns.","tags":[],"title":"Practice makes perfect? Detecting anomalies by learning from imperfect user’s feedback","type":"thesis"},{"authors":[],"categories":[],"content":"","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609459200,"objectID":"4c365661862aebbc874ee0ade697c817","permalink":"https://lorenzo-perini.github.io/thesis/3year/first/","publishdate":"2021-01-01T00:00:00Z","relpermalink":"/thesis/3year/first/","section":"thesis","summary":"The challenge of abstaining from uncertain predictions has gained significant attention in recent years. While the introduction of a reject option has been explored in supervised learning, its application in anomaly detection—a domain with limited labels and high costs for misclassification—remains unexplored. This work proposes a novel framework enabling anomaly detectors to abstain from uncertain predictions in both unsupervised and semi-supervised scenarios. The approach leverages a dependent rejector based on model confidence, making it adaptable to various anomaly detection methods. In the unsupervised setting, a natural threshold is used for rejection, whereas in the semi-supervised case, the threshold is optimized using labeled data to minimize overall costs. Additionally, cosine distance is employed to measure the reward of using labels for either Active Learning or Learning to Reject, balancing their trade-offs. Experiments on a benchmark of nine anomaly detection datasets demonstrate the framework’s effectiveness, showing significant improvements in rejecting high-cost misclassifications. The proposed framework, integrating rejection, outperforms standard Active Learning approaches in both unsupervised and semi-supervised settings, reducing risk and enhancing reliability.","tags":[],"title":"To ask or to abstain, what is the best strategy? Finding the best trade-off between Active Learning and Learning to Reject","type":"thesis"},{"authors":["Lorenzo Perini","Connor Galvin","Vincent Vercruyssen"],"categories":[],"content":"","date":1600041600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1600041600,"objectID":"a719ab9dd8683d5b8550a2853736de62","permalink":"https://lorenzo-perini.github.io/publication/edml20/","publishdate":"2020-08-21T12:35:55+02:00","relpermalink":"/publication/edml20/","section":"publication","summary":"Anomaly detection attempts to learn models from data that can detect anomalous examples in the data. However, naturally occurring variations in the data impact the model that is learned and thus which examples it will predict to be anomalies. Ideally, an anomaly detection method should be robust to such small changes in the data. Hence, this paper introduces a ranking stability measure that quantifies the robustness of any anomaly detector's predictions by looking at how consistently it ranks examples in terms of their anomalousness. Our experiments investigate the performance of this stability measure under different data perturbation schemes. In addition, they show how the stability measure can complement traditional anomaly detection performance measures, such as area under the ROC curve or average precision, to quantify the behaviour of different anomaly detection methods.","tags":["Ranking Stability","Anomaly Detection","Classifier Trust"],"title":"A Ranking Stability Measure for Quantifying the Robustness of Anomaly Detection Methods","type":"publication"},{"authors":["Lorenzo Perini","Vincent Vercruyssen","Jesse Davis"],"categories":[],"content":"","date":1600041600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1600041600,"objectID":"2ce36d5587162cbb092eaf84c4633b39","permalink":"https://lorenzo-perini.github.io/publication/ecml20/","publishdate":"2020-08-21T12:25:53+02:00","relpermalink":"/publication/ecml20/","section":"publication","summary":"Anomaly detection focuses on identifying examples in the data that somehow deviate from what is expected or typical. Algorithms for this task usually assign a score to each example that represents how anomalous the example is. Then, a threshold on the scores turns them into concrete predictions. However, each algorithm uses a different approach to assign the scores, which makes them difficult to interpret and can quickly erode a user's trust in the predictions. This paper introduces an approach for assessing the reliability of any anomaly detector's example-wise predictions. To do so, we propose a Bayesian approach for converting anomaly scores to probability estimates. This enables the anomaly detector to assign a confidence score to each prediction which captures its uncertainty in that prediction. We theoretically analyze the convergence behaviour of our confidence estimate. Empirically, we demonstrate the effectiveness of the framework in quantifying a detector's confidence in its predictions on a large benchmark of datasets.","tags":["Anomaly Detection","Interpretability","Confidence Scores"],"title":"Quantifying the Confidence of Anomaly Detectors in Their Example-Wise Predictions","type":"publication"},{"authors":[],"categories":[],"content":"","date":1598256914,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1598256914,"objectID":"d5ef38474751715cc3dea6d152772ce0","permalink":"https://lorenzo-perini.github.io/course/pulearning/","publishdate":"2020-08-24T10:15:14+02:00","relpermalink":"/course/pulearning/","section":"course","summary":"2020/2021, 2021/2022","tags":[],"title":"Capita Selecta Computer Science: PU Learning","type":"course"},{"authors":[],"categories":[],"content":"","date":1598256914,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1598256914,"objectID":"bdba931ff2a5f4a110fe4bbb0d893e9f","permalink":"https://lorenzo-perini.github.io/course/datamining/","publishdate":"2020-08-24T10:15:14+02:00","relpermalink":"/course/datamining/","section":"course","summary":"2019/2020, 2020/2021, 2022/2023","tags":[],"title":"Data Mining","type":"course"},{"authors":[],"categories":[],"content":"","date":1598256676,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1598256676,"objectID":"498d13916311804fdf6ebcf150c6f020","permalink":"https://lorenzo-perini.github.io/award/ecmlpkdd20/","publishdate":"2020-08-24T10:11:16+02:00","relpermalink":"/award/ecmlpkdd20/","section":"award","summary":"Multiple nominations by the conference session chairs as particularly engaging speaker.","tags":[],"title":"Overall ECML-PKDD Engagement Award 2020","type":"award"},{"authors":["Lorenzo Perini","Vincent Vercruyssen","Jesse Davis"],"categories":[],"content":"","date":1594771200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1594771200,"objectID":"300e9157a6182156b89161d233b14e6b","permalink":"https://lorenzo-perini.github.io/publication/ijcai20/","publishdate":"2020-07-15T00:00:00Z","relpermalink":"/publication/ijcai20/","section":"publication","summary":"Estimating the proportion of positive examples (i.e., the class prior) from positive and unlabeled (PU) data is an important task that facilitates learning a classifier from such data. In this paper, we explore how to tackle this problem when the observed labels were acquired via active learning. This introduces the challenge that the observed labels were not selected completely at random, which is the primary assumption underpinning existing approaches to estimating the class prior from PU data. We analyze this new setting and design an algorithm that is able to estimate the class prior for a given active learning strategy. Empirically, we show that our approach accurately recovers the true class prior on a benchmark of anomaly detection datasets and that it does so more accurately than existing methods.","tags":["PU Learning","Active Learning","Anomaly Detection"],"title":"Class Prior Estimation in Active Positive and Unlabeled Learning","type":"publication"},{"authors":[],"categories":[],"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577836800,"objectID":"e654b6c3183702c343954e9df7cce8b7","permalink":"https://lorenzo-perini.github.io/thesis/2year/first/","publishdate":"2020-01-01T00:00:00Z","relpermalink":"/thesis/2year/first/","section":"thesis","summary":"Anomaly detection identifies patterns in datasets that deviate from expected behavior, often indicating issues such as fraud, accidents, or intrusions. Due to the size of modern datasets, manual inspection is impractical, necessitating automated methods. This thesis explores anomaly detection using active learning, where a human expert provides annotations for selected data points. Existing methods assume experts can always assign correct labels, but this is unrealistic in practice. We propose a weaker assumption, allowing experts to express uncertainty when unsure about a label, reducing reliance on omniscient annotations. Our approach avoids overly difficult queries by estimating their difficulty and incorporating this estimate into the querying process alongside model uncertainty. Through experiments, we evaluate methods to estimate expert uncertainty, optimize query strategies, and minimize unnecessary queries. To address the lack of datasets with expert uncertainty, we modeled uncertainty on existing anomaly datasets, enabling an evaluation of the proposed framework.","tags":[],"title":"Do you know the answer? Taking into account the user uncertainty in active learning","type":"thesis"},{"authors":[],"categories":[],"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577836800,"objectID":"fa67618809b1f7e5f33abdbafc04fb70","permalink":"https://lorenzo-perini.github.io/thesis/2year/second/","publishdate":"2020-01-01T00:00:00Z","relpermalink":"/thesis/2year/second/","section":"thesis","summary":"This thesis investigates the application of active learning in anomaly detection, with a focus on the impact of dataset perturbations on query selection. Specifically, we examine the probability of querying a data point x when the dataset is slightly altered. This probability, termed the reliability measure, distinguishes data points that provide meaningful insights into the underlying data distribution from those queried due to unique dataset-specific characteristics. By estimating this probability, we can refine query selection to align more closely with the true data distribution rather than dataset-specific anomalies. The reliability measure also enhances the calibration and interpretability of informativeness scores used in active learning strategies. In the second part of the thesis, we explore combining active learning strategies by leveraging their reliability measures. This approach integrates the assumptions underlying the input strategies, though we find that combining assumptions does not always produce a superior strategy. The proposed framework provides a new perspective on optimizing active learning in anomaly detection through reliability-informed strategies.","tags":[],"title":"Reliability measure in the Active Learning querying phase","type":"thesis"},{"authors":[],"categories":[],"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"d32de50d40e4fd5995af286531e188cb","permalink":"https://lorenzo-perini.github.io/thesis/1year/","publishdate":"2019-01-01T00:00:00Z","relpermalink":"/thesis/1year/","section":"thesis","summary":"The idea of stability has applications in many areas of machine learning. However, stability has not yet been applied in context of anomaly detection, where there is need for a metric that quantifies the robustness of anomaly score rankings to changes in training data. We propose such a metric and the methodology used in computing it. We then propose to use an algorithm with the goal of maximizing stability by learning training points that contribute most to high stability. Finally, we apply this stability metric and the proposed contribution update algorithm on several benchmark datasets. This evaluation is used to compare stability on different anomaly detection algorithms, and assess the contribution update algorithm's ability to increase stability.","tags":[],"title":"Designing a Stability Metric for Assessing the Robustness of Anomaly Rankings","type":"thesis"},{"authors":[],"categories":[],"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"256f085db9b42d9be9674b4669f81f40","permalink":"https://lorenzo-perini.github.io/reviewer/aaai/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/reviewer/aaai/","section":"reviewer","summary":"Conference","tags":[],"title":"AAAI Conference on Artificial Intelligence (AAAI-21, AAAI-22)","type":"reviewer"},{"authors":[],"categories":[],"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"cbba21c7d05c83ac5e4303514dc65875","permalink":"https://lorenzo-perini.github.io/reviewer/kdd/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/reviewer/kdd/","section":"reviewer","summary":"Conference","tags":[],"title":"ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD-21, KDD-23)","type":"reviewer"},{"authors":[],"categories":[],"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"6bad1e87e8fced13250c39179ae29bc2","permalink":"https://lorenzo-perini.github.io/reviewer/mlj/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/reviewer/mlj/","section":"reviewer","summary":"Journal","tags":[],"title":"Editorial Board Member of Machine Learning Journal (MLJ)","type":"reviewer"},{"authors":[],"categories":[],"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"5cb30cd5335bb5d05f5ef321ea7b0308","permalink":"https://lorenzo-perini.github.io/reviewer/ecml/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/reviewer/ecml/","section":"reviewer","summary":"Conference","tags":[],"title":"European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases (ECML-20, ECML-22)","type":"reviewer"},{"authors":[],"categories":[],"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"8b392075b975480f7a4cc2257f031f7e","permalink":"https://lorenzo-perini.github.io/reviewer/aistats/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/reviewer/aistats/","section":"reviewer","summary":"Conference","tags":[],"title":"International Conference on Artificial Intelligence and Statistics (AISTATS-23)","type":"reviewer"},{"authors":[],"categories":[],"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"af4491e277020e9d3b1708989d41e20d","permalink":"https://lorenzo-perini.github.io/reviewer/jair/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/reviewer/jair/","section":"reviewer","summary":"","tags":[],"title":"Journal of Artificial Intelligence Research (JAIR)","type":"reviewer"},{"authors":[],"categories":[],"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"9e73f7dce02f6a8a47993dd53efcde13","permalink":"https://lorenzo-perini.github.io/reviewer/neurips/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/reviewer/neurips/","section":"reviewer","summary":"Conference","tags":[],"title":"Neural Information Processing Systems (NeurIPS-24)","type":"reviewer"},{"authors":[],"categories":[],"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"141c50570e8a05106b89515f08c1c5b6","permalink":"https://lorenzo-perini.github.io/reviewer/sdm/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/reviewer/sdm/","section":"reviewer","summary":"Conference","tags":[],"title":"SIAM International Conference on Data Mining (SDM-22)","type":"reviewer"}]